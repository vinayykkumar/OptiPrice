{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "data = pd.read_csv('/content/merged_data.csv')\n",
    "\n",
    "# Convert 'Date of Travel' to datetime and extract components\n",
    "data['Date of Travel'] = pd.to_datetime(data['Date of Travel'])\n",
    "data['Travel_Year'] = data['Date of Travel'].dt.year\n",
    "data['Travel_Month'] = data['Date of Travel'].dt.month\n",
    "data['Travel_Day'] = data['Date of Travel'].dt.day\n",
    "data = data.drop(columns=['Date of Travel'])\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = ['Company', 'City', 'Payment_Mode', 'Gender']  # Check if 'Month' is here\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in data.columns:  # Ensure column exists before encoding\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Verify and drop 'Month' if it exists\n",
    "if 'Month' in data.columns:\n",
    "    print(\"Warning: 'Month' column found and will be dropped as it’s redundant with 'Travel_Month'.\")\n",
    "    data = data.drop(columns=['Month'])\n",
    "\n",
    "# Feature engineering: Add interaction terms\n",
    "data['City_Distance'] = data['City'] * data['Distance Travelled(KM)']\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "exclude_cols = ['Profit', 'Price Charged', 'Cost of Trip', 'Income (USD/Month)', 'Age', 'Year', 'Payment_Mode', 'Gender']\n",
    "X = data.drop(columns=exclude_cols)\n",
    "y = data['Profit']\n",
    "\n",
    "# Check data types and convert if necessary\n",
    "print(\"\\nData types of features:\")\n",
    "print(X.dtypes)\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        raise ValueError(f\"Column {col} has object dtype, which is not supported. Please encode or drop it.\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, random_state=42)  # 70% train, 10% val, 20% test\n",
    "\n",
    "# Update categorical columns\n",
    "categorical_cols = ['Company', 'City']\n",
    "\n",
    "# Step 2: Hyperparameter tuning with RandomizedSearchCV for speed\n",
    "print(\"\\nPerforming Hyperparameter Tuning with RandomizedSearchCV...\")\n",
    "gbm = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    boosting_type='gbdt',\n",
    "    bagging_fraction=0.9,\n",
    "    feature_fraction=0.85,\n",
    "    bagging_freq=5,\n",
    "    subsample=0.8,  # Speed boost without reducing rows\n",
    "    verbose=-1,\n",
    "    n_jobs=-1,\n",
    "    lambda_l1=0.1,\n",
    "    lambda_l2=0.1\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves': [30, 40, 50],\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'min_child_samples': [20, 30, 40],\n",
    "    'n_estimators': [500, 1000]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=gbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Reduced combinations for speed\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise',  # To debug errors\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rand_search.fit(X_train, y_train)\n",
    "best_params = rand_search.best_params_\n",
    "print(\"\\nBest Hyperparameters from Randomized Search:\")\n",
    "print(best_params)\n",
    "print(f\"Best MSE (negative): {rand_search.best_score_:.2f}\")\n",
    "\n",
    "# Step 3: Train the final model with best parameters and validation\n",
    "final_params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'bagging_fraction': 0.9,\n",
    "    'feature_fraction': 0.85,\n",
    "    'bagging_freq': 5,\n",
    "    'subsample': 0.8,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    **best_params\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "\n",
    "print(\"\\nTraining Final LightGBM Model with Best Parameters...\")\n",
    "final_gbm = lgb.train(\n",
    "    final_params,\n",
    "    train_data,\n",
    "    num_boost_round=best_params['n_estimators'],\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20), lgb.log_evaluation(period=10)]\n",
    ")\n",
    "\n",
    "# Step 4: Evaluate on test set\n",
    "y_pred = final_gbm.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate tolerance-based accuracy\n",
    "tolerance = 50\n",
    "absolute_errors = np.abs(y_pred - y_test)\n",
    "accuracy_within_tolerance = np.mean(absolute_errors <= tolerance) * 100\n",
    "\n",
    "print(\"\\nFinal LightGBM Performance on Test Data:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "print(f\"Root Mean Squared Error (per trip): ${rmse:.2f}\")\n",
    "print(f\"Accuracy (within ±$50 tolerance): {accuracy_within_tolerance:.2f}%\")\n",
    "\n",
    "# Step 5: Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': final_gbm.feature_importance(importance_type='gain')\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFinal LightGBM Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Step 6: Function to generate synthetic data with 2% inflation\n",
    "def generate_future_data(base_data, year, month, num_trips):\n",
    "    historical_month_data = base_data[base_data['Travel_Month'] == month].copy()\n",
    "    if historical_month_data.empty:\n",
    "        historical_month_data = base_data.copy()\n",
    "    \n",
    "    future_data = historical_month_data.sample(n=num_trips, replace=True, random_state=42).copy()\n",
    "    future_data['Travel_Year'] = year\n",
    "    future_data['Travel_Month'] = month\n",
    "    future_data['Travel_Day'] = np.random.randint(1, 31, size=num_trips)\n",
    "    years_diff = year - 2018\n",
    "    future_data['Price Charged'] *= (1 + 0.02) ** years_diff\n",
    "    future_data['Cost of Trip'] *= (1 + 0.02) ** years_diff\n",
    "    future_data['City_Distance'] = future_data['City'] * future_data['Distance Travelled(KM)']\n",
    "    future_X = future_data.drop(columns=exclude_cols)\n",
    "    return future_X, future_data\n",
    "\n",
    "avg_trips_per_month = len(data) // (data['Travel_Year'].nunique() * 12)\n",
    "print(f\"\\nAverage Trips per Month: {avg_trips_per_month}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_year = int(input(\"Enter the year for profit prediction (e.g., 2025): \"))\n",
    "        user_month = int(input(\"Enter the month for profit prediction (1-12): \"))\n",
    "        if 1 <= user_month <= 12 and user_year >= 2019:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Month must be 1-12, and year must be 2019 or later.\")\n",
    "    except ValueError:\n",
    "        print(\"Please enter valid numeric values.\")\n",
    "\n",
    "# Generate future data and predict profits\n",
    "future_X, future_data = generate_future_data(data, user_year, user_month, avg_trips_per_month)\n",
    "future_pred_profits = final_gbm.predict(future_X)\n",
    "\n",
    "# Total profit with confidence interval\n",
    "total_future_profit = future_pred_profits.sum()\n",
    "total_error = rmse * avg_trips_per_month\n",
    "print(f\"\\nPredicted Total Company Profit for {user_month}/{user_year}: ${total_future_profit:,.2f}\")\n",
    "print(f\"Estimated Range: ${total_future_profit - total_error:,.2f}–${total_future_profit + total_error:,.2f}\")\n",
    "print(f\"(Note: Range based on RMSE ±${rmse:.2f} per trip across {avg_trips_per_month} trips)\")\n",
    "\n",
    "# Breakdown by company\n",
    "future_data['Predicted_Profit'] = future_pred_profits\n",
    "future_data['Company'] = label_encoders['Company'].inverse_transform(future_data['Company'])\n",
    "profit_by_company = future_data.groupby('Company')['Predicted_Profit'].sum()\n",
    "pink_cab_profit = profit_by_company.get('Pink Cab', 0)\n",
    "yellow_cab_profit = profit_by_company.get('Yellow Cab', 0)\n",
    "pink_error = total_error * (pink_cab_profit / total_future_profit) if total_future_profit != 0 else 0\n",
    "yellow_error = total_error * (yellow_cab_profit / total_future_profit) if total_future_profit != 0 else 0\n",
    "\n",
    "print(f\"\\nProfit Breakdown by Company for {user_month}/{user_year}:\")\n",
    "print(f\"Pink Cab: ${pink_cab_profit:,.2f} (Range: ${pink_cab_profit - pink_error:,.2f}–${pink_cab_profit + pink_error:,.2f})\")\n",
    "print(f\"Yellow Cab: ${yellow_cab_profit:,.2f} (Range: ${yellow_cab_profit - yellow_error:,.2f}–${yellow_cab_profit + yellow_error:,.2f})\")\n",
    "print(f\"Total (Pink Cab + Yellow Cab): ${(pink_cab_profit + yellow_cab_profit):,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
